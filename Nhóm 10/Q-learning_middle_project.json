{"paragraphs":[{"text":"%sh\r\npip install matplotlib\r\npip install numpy \r\npip install seaborn\r\npip install tqdm\r\npip install gymnasium==0.27.0\r\npip install gymnasium[toy_text]","user":"anonymous","dateUpdated":"2023-04-29T13:46:53+0700","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":16,"editorHide":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1682750813541_-50891056","id":"20230412-155036_13518691","dateCreated":"2023-04-29T13:46:53+0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:979"},{"text":"%pyspark\n\nfrom __future__ import annotations\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib.patches import Patch\nfrom tqdm import tqdm\nimport gymnasium as gym\n\nenv = gym.make(\"Blackjack-v1\", sab=True,render_mode=\"rgb_array\")\nprint(env.observation_space)\nprint(env.action_space)","user":"anonymous","dateUpdated":"2023-04-29T13:46:53+0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":16,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Tuple(Discrete(32), Discrete(11), Discrete(2))\nDiscrete(2)\n"}]},"apps":[],"jobName":"paragraph_1682750813544_1910914625","id":"20230412-155122_18094974","dateCreated":"2023-04-29T13:46:53+0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:980"},{"text":"%pyspark\nclass BlackjackAgent:\n    def __init__(\n        self,\n        learning_rate: float,\n        initial_epsilon: float,\n        epsilon_decay: float,\n        final_epsilon: float,\n        discount_factor: float = 0.95,\n    ):\n        \"\"\"\n        Khởi tạo tác tử học tăng cường với Q-table (một dictionary rỗng chứa giá trị của các cặp hành động - trạng thái) \n        và các tham số:\n            learning_rate: Tỷ lệ học tập của tác tử\n            initial_epsilon: Giá trị epsilon ban đầu\n            epsilon_decay: Tỷ lệ giảm dần của epsilon\n            final_epsilon: Giá trị cuối cùng của epsilon\n            discount_factor: Hệ số chiết khấu, được sử dụng để xác định mức độ ảnh hưởng của các phần thưởng trong tương lai \n                            đối với quyết định hiện tại\n        \"\"\"\n        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n\n        self.lr = learning_rate\n        self.discount_factor = discount_factor\n\n        self.epsilon = initial_epsilon\n        self.epsilon_decay = epsilon_decay\n        self.final_epsilon = final_epsilon\n\n        self.training_error = []\n\n    def get_action(self, obs: tuple[int, int, bool]) -> int:\n        \"\"\"\n        Trả về hành động tốt nhất với tỷ lệ (1 - epsilon)\n        hoặc một hành động ngẫu nhiên với tỷ lệ epsilon để đảm bảo tính khám phá\n        \"\"\"\n        # với tỷ lệ epsilon trả về hành động ngẫu nhiên để khám phá môi trường\n        if np.random.random() < self.epsilon:\n            return env.action_space.sample()\n\n        # với tỷ lệ (1 - epsilon) thực hiện hành động tham lam (khai thác)\n        else:\n            return int(np.argmax(self.q_values[obs]))\n\n    def update(\n        self,\n        obs: tuple[int, int, bool],\n        action: int,\n        reward: float,\n        terminated: bool,\n        next_obs: tuple[int, int, bool],\n    ):\n        # cập nhật giá trị Q-value của cặp trạng thái - hành động\n        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n        temporal_difference = (\n            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n        )\n\n        self.q_values[obs][action] = (\n            self.q_values[obs][action] + self.lr * temporal_difference\n        )\n        self.training_error.append(temporal_difference)\n\n    def decay_epsilon(self):\n        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)","user":"anonymous","dateUpdated":"2023-04-29T13:46:53+0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":16,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1682750813546_-676964313","id":"20230412-155224_437123","dateCreated":"2023-04-29T13:46:53+0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:981"},{"title":"Building training loop","text":"%pyspark\n# Các tham số\nlearning_rate = 0.01\nn_episodes = 1000\nstart_epsilon = 1.0\nepsilon_decay = start_epsilon / (n_episodes / 2)  # giảm khám phá, tăng khai thác theo thời gian\nfinal_epsilon = 0.1\n\nagent = BlackjackAgent(\n    learning_rate=learning_rate,\n    initial_epsilon=start_epsilon,\n    epsilon_decay=epsilon_decay,\n    final_epsilon=final_epsilon,\n)\n\nfrom IPython.display import clear_output\nenv = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\nfor episode in tqdm(range(n_episodes)):\n    obs, info = env.reset()\n    done = False\n    clear_output()\n\n    # một vòng huấn luyện\n    while not done:\n        action = agent.get_action(obs)\n        print(f\"\\nCurrent obs = {obs}, \")\n        print(f\"Action do  = {action} \")\n        next_obs, reward, terminated, truncated, info = env.step(action)\n        print(f\"=> Next obs = {next_obs}, Reward = {reward}\")\n        # cập nhật giá trị q-value\n        agent.update(obs, action, reward, terminated, next_obs)\n        frame = env.render()\n        plt.imshow(frame)\n        plt.show()\n        # cập nhật trạng thái hiện tại và môi trường đã xong một vòng huấn luyện hay chưa\n        done = terminated or truncated\n        obs = next_obs\n        \n    #giảm tỷ lệ khám phá sau mỗi vòng huấn luyện\n    agent.decay_epsilon()","user":"anonymous","dateUpdated":"2023-04-29T13:50:00+0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":16,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.InterruptedException: sleep interrupted"}]},"apps":[],"jobName":"paragraph_1682750813548_-1823396940","id":"20230412-155239_25282733","dateCreated":"2023-04-29T13:46:53+0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:982"},{"text":"%pyspark\nrolling_length = 500\nfig, axs = plt.subplots(ncols=3, figsize=(12, 5))\naxs[0].set_title(\"Episode rewards\")\nreward_moving_average = (\n    np.convolve(\n        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode=\"valid\"\n    )\n    / rolling_length\n)\naxs[0].plot(range(len(reward_moving_average)), reward_moving_average)\naxs[1].set_title(\"Episode lengths\")\nlength_moving_average = (\n    np.convolve(\n        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode=\"same\"\n    )\n    / rolling_length\n)\naxs[1].plot(range(len(length_moving_average)), length_moving_average)\naxs[2].set_title(\"Training Error\")\ntraining_error_moving_average = (\n    np.convolve(np.array(agent.training_error), np.ones(rolling_length), mode=\"same\")\n    / rolling_length\n)\naxs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\nplt.tight_layout()\nplt.show()","user":"anonymous","dateUpdated":"2023-04-29T13:50:12+0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":12,"editorHide":false,"results":{},"enabled":true,"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1682750813549_-32323612","id":"20230422-154036_1195242092","dateCreated":"2023-04-29T13:46:53+0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:983","title":"Trực quan hoá quá trình training"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2023-04-29T13:46:53+0700","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1682750813550_-1247453651","id":"20230422-154122_1935476819","dateCreated":"2023-04-29T13:46:53+0700","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:984"}],"name":"Q-learning_middle_project","id":"2HXCC6A4Q","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}